{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fin_data import DailyTimeSeries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/home/joe2/Documents/LambdaSchool/labs_ir/repos/Data-Science/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/home/joe2/anaconda3/envs/riskylobster/lib/python36.zip', '/home/joe2/anaconda3/envs/riskylobster/lib/python3.6', '/home/joe2/anaconda3/envs/riskylobster/lib/python3.6/lib-dynload', '/home/joe2/anaconda3/envs/riskylobster/lib/python3.6/site-packages', '/home/joe2/anaconda3/envs/riskylobster/lib/python3.6/site-packages/IPython/extensions', '/home/joe2/.ipython']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################### \n",
      " Ticker:  AAPL \n",
      " Last Refreshed:  2019-08-28 \n",
      " Data Retrieved:  Daily Prices (open, high, low, close) and Volumes \n",
      " ###################################################################\n"
     ]
    }
   ],
   "source": [
    "apple = DailyTimeSeries('AAPL')\n",
    "dfaapl = apple.initiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['operatingrevenue',\n",
       " 'totalrevenue',\n",
       " 'operatingcostofrevenue',\n",
       " 'totalcostofrevenue',\n",
       " 'totalgrossprofit',\n",
       " 'sgaexpense',\n",
       " 'rdexpense',\n",
       " 'totaloperatingexpenses',\n",
       " 'totaloperatingincome',\n",
       " 'otherincome',\n",
       " 'totalotherincome',\n",
       " 'totalpretaxincome',\n",
       " 'incometaxexpense',\n",
       " 'netincomecontinuing',\n",
       " 'netincome',\n",
       " 'netincometocommon',\n",
       " 'weightedavebasicsharesos',\n",
       " 'basiceps',\n",
       " 'weightedavedilutedsharesos',\n",
       " 'dilutedeps',\n",
       " 'weightedavebasicdilutedsharesos',\n",
       " 'basicdilutedeps',\n",
       " 'cashdividendspershare',\n",
       " 'date',\n",
       " 'fiscal_year',\n",
       " 'quarter']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple.list_available_fundamentals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "funs = ['totalrevenue',  'totalcostofrevenue', 'totalgrossprofit', 'totalpretaxincome', 'weightedavebasicdilutedsharesos', 'cashdividendspershare', 'quarter']\n",
    "techs = ['SMA', 'WMA', 'STOCH', 'ROC', 'AROON']\n",
    "macros = ['housing_index', 'confidence_index', 'trade_index', 'longterm_rates', 'shortterm_rates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################### \n",
      " Ticker:  AAPL \n",
      " Fundamentals Retrieved:  ['AAPL open' 'AAPL high' 'AAPL low' 'AAPL close' 'AAPL volume'\n",
      " 'AAPL_totalrevenue' 'AAPL_totalcostofrevenue' 'AAPL_totalgrossprofit'\n",
      " 'AAPL_totalpretaxincome' 'AAPL_weightedavebasicdilutedsharesos'\n",
      " 'AAPL_cashdividendspershare' 'AAPL_quarter'] \n",
      " ###################################################################\n",
      "################################################################### \n",
      " Ticker:  AAPL \n",
      " Retrieved Data Start Date:  2009-07-22 \n",
      " Retrieved Data End Date:  2019-07-31 \n",
      " Data Retrieved:  ['AAPL_totalrevenue', 'AAPL_totalcostofrevenue', 'AAPL_totalgrossprofit', 'AAPL_totalpretaxincome', 'AAPL_weightedavebasicdilutedsharesos', 'AAPL_cashdividendspershare', 'AAPL_quarter'] \n",
      " ###################################################################\n"
     ]
    }
   ],
   "source": [
    "dfaapl = apple.add_fundamentals(dfaapl, funs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################### \n",
      " Ticker:  AAPL \n",
      " Last Refreshed:  Simple Moving Average (SMA) \n",
      " Data Retrieved:  2019-08-28 \n",
      " ###################################################################\n",
      "################################################################### \n",
      " Ticker:  AAPL \n",
      " Last Refreshed:  Weighted Moving Average (WMA) \n",
      " Data Retrieved:  2019-08-28 \n",
      " ###################################################################\n",
      "################################################################### \n",
      " Ticker:  AAPL \n",
      " Last Refreshed:  Stochastic (STOCH) \n",
      " Data Retrieved:  2019-08-28 \n",
      " ###################################################################\n",
      "################################################################### \n",
      " Ticker:  AAPL \n",
      " Last Refreshed:  Rate of change : ((price/prevPrice)-1)*100 \n",
      " Data Retrieved:  2019-08-28 \n",
      " ###################################################################\n",
      "################################################################### \n",
      " Ticker:  AAPL \n",
      " Last Refreshed:  Aroon (AROON) \n",
      " Data Retrieved:  2019-08-28 \n",
      " ###################################################################\n"
     ]
    }
   ],
   "source": [
    "dfaapl = apple.add_technicals(techs, dfaapl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################### \n",
      " Index: Nominal Home Price Index Added \n",
      " ###################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joe2/Documents/LambdaSchool/labs_ir/repos/Data-Science/data/fin_data.py:293: UserWarning: The latest value available for Housing Index is from January 2019.\n",
      "  warnings.warn(\"The latest value available for Housing Index is from January 2019.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################### \n",
      " Index: Yale Investor Behavior Project Added \n",
      " ###################################################################\n",
      "################################################################### \n",
      " Trade Weighted U.S. Dollar Index: Broad Added \n",
      " ###################################################################\n",
      "################################################################### \n",
      " US Treasury Bond Long-Term Rates Added \n",
      " ###################################################################\n",
      "################################################################### \n",
      " US Treasury Bond Short-Term Rates Added \n",
      " ###################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joe2/Documents/LambdaSchool/labs_ir/repos/Data-Science/data/fin_data.py:334: UserWarning: Contains Null Values\n",
      "  warnings.warn(\"Contains Null Values\")\n"
     ]
    }
   ],
   "source": [
    "dfaapl = apple.add_macro(dfaapl, macros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL open</th>\n",
       "      <th>AAPL high</th>\n",
       "      <th>AAPL low</th>\n",
       "      <th>AAPL close</th>\n",
       "      <th>AAPL volume</th>\n",
       "      <th>AAPL_totalrevenue</th>\n",
       "      <th>AAPL_totalcostofrevenue</th>\n",
       "      <th>AAPL_totalgrossprofit</th>\n",
       "      <th>AAPL_totalpretaxincome</th>\n",
       "      <th>AAPL_weightedavebasicdilutedsharesos</th>\n",
       "      <th>...</th>\n",
       "      <th>4_Wk_DR</th>\n",
       "      <th>4_Wk_CE</th>\n",
       "      <th>8_Wk_DR</th>\n",
       "      <th>8_Wk_CE</th>\n",
       "      <th>13_Wk_DR</th>\n",
       "      <th>13_Wk_CE</th>\n",
       "      <th>26_Wk_DR</th>\n",
       "      <th>26_Wk_CE</th>\n",
       "      <th>52_Wk_DR</th>\n",
       "      <th>52_Wk_CE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2019-01-31</td>\n",
       "      <td>166.11</td>\n",
       "      <td>169.00</td>\n",
       "      <td>164.56</td>\n",
       "      <td>166.44</td>\n",
       "      <td>40739600.0</td>\n",
       "      <td>8.829300e+10</td>\n",
       "      <td>5.438100e+10</td>\n",
       "      <td>3.391200e+10</td>\n",
       "      <td>2.703000e+10</td>\n",
       "      <td>5.118600e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.42</td>\n",
       "      <td>2.39</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.47</td>\n",
       "      <td>2.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-01-30</td>\n",
       "      <td>163.25</td>\n",
       "      <td>166.15</td>\n",
       "      <td>160.23</td>\n",
       "      <td>165.25</td>\n",
       "      <td>61109800.0</td>\n",
       "      <td>8.829300e+10</td>\n",
       "      <td>5.438100e+10</td>\n",
       "      <td>3.391200e+10</td>\n",
       "      <td>2.703000e+10</td>\n",
       "      <td>5.118600e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.39</td>\n",
       "      <td>2.37</td>\n",
       "      <td>2.42</td>\n",
       "      <td>2.44</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.49</td>\n",
       "      <td>2.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-01-29</td>\n",
       "      <td>156.25</td>\n",
       "      <td>158.13</td>\n",
       "      <td>154.11</td>\n",
       "      <td>154.68</td>\n",
       "      <td>41587200.0</td>\n",
       "      <td>4.540800e+10</td>\n",
       "      <td>2.792000e+10</td>\n",
       "      <td>1.748800e+10</td>\n",
       "      <td>1.130800e+10</td>\n",
       "      <td>5.188700e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.39</td>\n",
       "      <td>2.37</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.37</td>\n",
       "      <td>2.42</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.52</td>\n",
       "      <td>2.52</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-01-28</td>\n",
       "      <td>155.79</td>\n",
       "      <td>156.33</td>\n",
       "      <td>153.66</td>\n",
       "      <td>156.30</td>\n",
       "      <td>26192100.0</td>\n",
       "      <td>4.540800e+10</td>\n",
       "      <td>2.792000e+10</td>\n",
       "      <td>1.748800e+10</td>\n",
       "      <td>1.130800e+10</td>\n",
       "      <td>5.188700e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.39</td>\n",
       "      <td>2.37</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.37</td>\n",
       "      <td>2.42</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.52</td>\n",
       "      <td>2.52</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-01-25</td>\n",
       "      <td>155.48</td>\n",
       "      <td>158.13</td>\n",
       "      <td>154.32</td>\n",
       "      <td>157.76</td>\n",
       "      <td>33535500.0</td>\n",
       "      <td>4.540800e+10</td>\n",
       "      <td>2.792000e+10</td>\n",
       "      <td>1.748800e+10</td>\n",
       "      <td>1.130800e+10</td>\n",
       "      <td>5.188700e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.32</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.37</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.34</td>\n",
       "      <td>2.39</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.51</td>\n",
       "      <td>2.52</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            AAPL open  AAPL high  AAPL low  AAPL close  AAPL volume  \\\n",
       "date                                                                  \n",
       "2019-01-31     166.11     169.00    164.56      166.44   40739600.0   \n",
       "2019-01-30     163.25     166.15    160.23      165.25   61109800.0   \n",
       "2019-01-29     156.25     158.13    154.11      154.68   41587200.0   \n",
       "2019-01-28     155.79     156.33    153.66      156.30   26192100.0   \n",
       "2019-01-25     155.48     158.13    154.32      157.76   33535500.0   \n",
       "\n",
       "            AAPL_totalrevenue  AAPL_totalcostofrevenue  AAPL_totalgrossprofit  \\\n",
       "date                                                                            \n",
       "2019-01-31       8.829300e+10             5.438100e+10           3.391200e+10   \n",
       "2019-01-30       8.829300e+10             5.438100e+10           3.391200e+10   \n",
       "2019-01-29       4.540800e+10             2.792000e+10           1.748800e+10   \n",
       "2019-01-28       4.540800e+10             2.792000e+10           1.748800e+10   \n",
       "2019-01-25       4.540800e+10             2.792000e+10           1.748800e+10   \n",
       "\n",
       "            AAPL_totalpretaxincome  AAPL_weightedavebasicdilutedsharesos  ...  \\\n",
       "date                                                                      ...   \n",
       "2019-01-31            2.703000e+10                          5.118600e+09  ...   \n",
       "2019-01-30            2.703000e+10                          5.118600e+09  ...   \n",
       "2019-01-29            1.130800e+10                          5.188700e+09  ...   \n",
       "2019-01-28            1.130800e+10                          5.188700e+09  ...   \n",
       "2019-01-25            1.130800e+10                          5.188700e+09  ...   \n",
       "\n",
       "            4_Wk_DR 4_Wk_CE  8_Wk_DR  8_Wk_CE  13_Wk_DR  13_Wk_CE  26_Wk_DR  \\\n",
       "date                                                                          \n",
       "2019-01-31     2.38    2.42     2.39     2.43      2.36      2.41      2.40   \n",
       "2019-01-30     2.36    2.40     2.35     2.39      2.37      2.42      2.44   \n",
       "2019-01-29     2.35    2.39     2.37     2.41      2.37      2.42      2.45   \n",
       "2019-01-28     2.35    2.39     2.37     2.41      2.37      2.42      2.45   \n",
       "2019-01-25     2.32    2.36     2.37     2.41      2.34      2.39      2.45   \n",
       "\n",
       "            26_Wk_CE  52_Wk_DR  52_Wk_CE  \n",
       "date                                      \n",
       "2019-01-31      2.46      2.47      2.55  \n",
       "2019-01-30      2.50      2.49      2.57  \n",
       "2019-01-29      2.52      2.52      2.60  \n",
       "2019-01-28      2.52      2.52      2.60  \n",
       "2019-01-25      2.51      2.52      2.60  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfaapl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4300, 35)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfaapl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AAPL open                                  0\n",
       "AAPL high                                  0\n",
       "AAPL low                                   0\n",
       "AAPL close                                 0\n",
       "AAPL volume                                0\n",
       "AAPL_totalrevenue                       1901\n",
       "AAPL_totalcostofrevenue                 1901\n",
       "AAPL_totalgrossprofit                   1901\n",
       "AAPL_totalpretaxincome                  1901\n",
       "AAPL_weightedavebasicdilutedsharesos    1901\n",
       "AAPL_cashdividendspershare              1901\n",
       "AAPL_quarter                            1901\n",
       "AAPL_SMA                                   0\n",
       "AAPL_WMA                                   0\n",
       "AAPL_SlowD                                 0\n",
       "AAPL_SlowK                                 0\n",
       "AAPL_ROC                                   0\n",
       "AAPL_Aroon Down                            0\n",
       "AAPL_Aroon Up                              0\n",
       "housing_index                              0\n",
       "conf_index                                 0\n",
       "conf_index_SE                              0\n",
       "trade_value                                0\n",
       "10 Yrs Rates                               0\n",
       "20-Yr Maturity Rate                        0\n",
       "4_Wk_DR                                    1\n",
       "4_Wk_CE                                    1\n",
       "8_Wk_DR                                 4227\n",
       "8_Wk_CE                                 4227\n",
       "13_Wk_DR                                   0\n",
       "13_Wk_CE                                   0\n",
       "26_Wk_DR                                   0\n",
       "26_Wk_CE                                   0\n",
       "52_Wk_DR                                1610\n",
       "52_Wk_CE                                1610\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfaapl.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfaapl = dfaapl.drop(labels=['8_Wk_DR', '8_Wk_CE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfaapl_clean = dfaapl.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2398, 33)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfaapl_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "mMscale = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_tar = mMscale.fit_transform(dfaapl_clean[['AAPL open', 'AAPL close', 'AAPL high', 'AAPL low', 'AAPL volume']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(pca_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.49703139  0.49704388  0.49985896  0.49351104  0.11178125]\n",
      " [-0.05234709 -0.05729527 -0.04322941 -0.07082408  0.99352514]]\n"
     ]
    }
   ],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31576651 0.01302855]\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_comp = pca.transform(pca_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcatar_aapl = dfaapl_clean[['AAPL_totalrevenue', 'AAPL_totalcostofrevenue', 'AAPL_totalgrossprofit',\n",
    "       'AAPL_totalpretaxincome', 'AAPL_weightedavebasicdilutedsharesos',\n",
    "       'AAPL_cashdividendspershare', 'AAPL_SMA', 'AAPL_WMA',\n",
    "       'AAPL_SlowK', 'AAPL_SlowD', 'AAPL_ROC', 'AAPL_Aroon Up',\n",
    "       'AAPL_Aroon Down', 'housing_index', 'conf_index', 'conf_index_SE',\n",
    "       'trade_value', '10 Yrs Rates', '20-Yr Maturity Rate', '4_Wk_DR',\n",
    "       '4_Wk_CE', '13_Wk_DR', '13_Wk_CE', '26_Wk_DR', '26_Wk_CE', '52_Wk_DR',\n",
    "       '52_Wk_CE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AAPL open', 'AAPL high', 'AAPL low', 'AAPL close', 'AAPL volume',\n",
       "       'AAPL_totalrevenue', 'AAPL_totalcostofrevenue', 'AAPL_totalgrossprofit',\n",
       "       'AAPL_totalpretaxincome', 'AAPL_weightedavebasicdilutedsharesos',\n",
       "       'AAPL_cashdividendspershare', 'AAPL_quarter', 'AAPL_SMA', 'AAPL_WMA',\n",
       "       'AAPL_SlowD', 'AAPL_SlowK', 'AAPL_ROC', 'AAPL_Aroon Down',\n",
       "       'AAPL_Aroon Up', 'housing_index', 'conf_index', 'conf_index_SE',\n",
       "       'trade_value', '10 Yrs Rates', '20-Yr Maturity Rate', '4_Wk_DR',\n",
       "       '4_Wk_CE', '13_Wk_DR', '13_Wk_CE', '26_Wk_DR', '26_Wk_CE', '52_Wk_DR',\n",
       "       '52_Wk_CE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfaapl_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.19 , -5.88 , -4.475, ..., -1.14 , -1.623, -1.08 ])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.gradient(dfaapl_clean['AAPL close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfaapl_clean = dfaapl_clean.drop('AAPL_quarter', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AAPL open                               float64\n",
       "AAPL high                               float64\n",
       "AAPL low                                float64\n",
       "AAPL close                              float64\n",
       "AAPL volume                             float64\n",
       "AAPL_totalrevenue                       float64\n",
       "AAPL_totalcostofrevenue                 float64\n",
       "AAPL_totalgrossprofit                   float64\n",
       "AAPL_totalpretaxincome                  float64\n",
       "AAPL_weightedavebasicdilutedsharesos    float64\n",
       "AAPL_cashdividendspershare              float64\n",
       "AAPL_SMA                                float64\n",
       "AAPL_WMA                                float64\n",
       "AAPL_SlowD                              float64\n",
       "AAPL_SlowK                              float64\n",
       "AAPL_ROC                                float64\n",
       "AAPL_Aroon Down                         float64\n",
       "AAPL_Aroon Up                           float64\n",
       "housing_index                           float64\n",
       "conf_index                              float64\n",
       "conf_index_SE                           float64\n",
       "trade_value                             float64\n",
       "10 Yrs Rates                            float64\n",
       "20-Yr Maturity Rate                     float64\n",
       "4_Wk_DR                                 float64\n",
       "4_Wk_CE                                 float64\n",
       "13_Wk_DR                                float64\n",
       "13_Wk_CE                                float64\n",
       "26_Wk_DR                                float64\n",
       "26_Wk_CE                                float64\n",
       "52_Wk_DR                                float64\n",
       "52_Wk_CE                                float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfaapl_clean.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate multi-step stacked lstm example\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['K',\n",
       " 'KLD',\n",
       " 'MAE',\n",
       " 'MAPE',\n",
       " 'MSE',\n",
       " 'MSLE',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'absolute_import',\n",
       " 'binary_crossentropy',\n",
       " 'categorical_crossentropy',\n",
       " 'categorical_hinge',\n",
       " 'cosine',\n",
       " 'cosine_proximity',\n",
       " 'deserialize',\n",
       " 'deserialize_keras_object',\n",
       " 'division',\n",
       " 'get',\n",
       " 'hinge',\n",
       " 'kld',\n",
       " 'kullback_leibler_divergence',\n",
       " 'logcosh',\n",
       " 'mae',\n",
       " 'mape',\n",
       " 'mean_absolute_error',\n",
       " 'mean_absolute_percentage_error',\n",
       " 'mean_squared_error',\n",
       " 'mean_squared_logarithmic_error',\n",
       " 'mse',\n",
       " 'msle',\n",
       " 'poisson',\n",
       " 'print_function',\n",
       " 'serialize',\n",
       " 'serialize_keras_object',\n",
       " 'six',\n",
       " 'sparse_categorical_crossentropy',\n",
       " 'squared_hinge']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define testing input sequence\n",
    "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
    "\n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 3, 2\n",
    "\n",
    "\n",
    "# covert into input/output\n",
    "X, y = split_sequences(dataset, n_steps_in, n_steps_out)\n",
    "\n",
    "# get number of features\n",
    "n_features = X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/joe2/anaconda3/envs/riskylobster/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='sigmoid', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "model.add(LSTM(100, activation='relu'))\n",
    "model.add(Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/joe2/anaconda3/envs/riskylobster/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f20b240fa20>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[187.13326 210.0655 ]]\n"
     ]
    }
   ],
   "source": [
    "# demonstrate prediction\n",
    "x_input = array([[70, 75], [80, 85], [90, 95]])\n",
    "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[166.11, 169.  , 164.56, ...,   2.46,   2.47,   2.55],\n",
       "       [163.25, 166.15, 160.23, ...,   2.5 ,   2.49,   2.57],\n",
       "       [156.25, 158.13, 154.11, ...,   2.52,   2.52,   2.6 ],\n",
       "       ...,\n",
       "       [156.95, 160.  , 156.5 , ...,   0.28,   0.43,   0.44],\n",
       "       [156.63, 158.44, 155.56, ...,   0.28,   0.45,   0.46],\n",
       "       [157.79, 158.73, 156.11, ...,   0.27,   0.43,   0.44]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfaapl_clean.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_columns = dfaapl_clean.columns\n",
    "X = {k:dfaapl_clean[k].to_numpy() for k in x_columns}\n",
    "y = np.gradient(dfaapl_clean['AAPL close'].to_numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in X.keys():\n",
    "    l = len(X[k])\n",
    "    X[k] = X[k].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_in = 21\n",
    "n_steps_out = 7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.hstack([X[k] for k in X.keys()] + [y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_split, y_split = split_sequences(dataset, n_steps_in=n_steps_in, n_steps_out=n_steps_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_split.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='sigmoid', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "model.add(LSTM(50, activation='sigmoid'))\n",
    "model.add(Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1897 samples, validate on 475 samples\n",
      "Epoch 1/500\n",
      " - 14s - loss: 98.3771 - val_loss: 8.9694\n",
      "Epoch 2/500\n",
      " - 8s - loss: 98.3276 - val_loss: 8.9662\n",
      "Epoch 3/500\n",
      " - 8s - loss: 98.3147 - val_loss: 8.9699\n",
      "Epoch 4/500\n",
      " - 9s - loss: 98.3043 - val_loss: 8.9728\n",
      "Epoch 5/500\n",
      " - 9s - loss: 98.2955 - val_loss: 8.9753\n",
      "Epoch 6/500\n",
      " - 9s - loss: 98.2876 - val_loss: 8.9777\n",
      "Epoch 7/500\n",
      " - 9s - loss: 98.2804 - val_loss: 8.9804\n",
      "Epoch 8/500\n",
      " - 9s - loss: 98.2736 - val_loss: 8.9834\n",
      "Epoch 9/500\n",
      " - 9s - loss: 98.2671 - val_loss: 8.9868\n",
      "Epoch 10/500\n",
      " - 9s - loss: 98.2607 - val_loss: 8.9906\n",
      "Epoch 11/500\n",
      " - 9s - loss: 98.2543 - val_loss: 8.9950\n",
      "Epoch 12/500\n",
      " - 9s - loss: 98.2479 - val_loss: 8.9999\n",
      "Epoch 13/500\n",
      " - 10s - loss: 98.2413 - val_loss: 9.0055\n",
      "Epoch 14/500\n",
      " - 10s - loss: 98.2344 - val_loss: 9.0115\n",
      "Epoch 15/500\n",
      " - 10s - loss: 98.2274 - val_loss: 9.0186\n",
      "Epoch 16/500\n",
      " - 9s - loss: 98.2201 - val_loss: 9.0272\n",
      "Epoch 17/500\n",
      " - 9s - loss: 98.2122 - val_loss: 9.0364\n",
      "Epoch 18/500\n",
      " - 9s - loss: 98.2042 - val_loss: 9.0488\n",
      "Epoch 19/500\n",
      " - 7s - loss: 98.1956 - val_loss: 9.0631\n",
      "Epoch 20/500\n",
      " - 7s - loss: 98.1866 - val_loss: 9.0809\n",
      "Epoch 21/500\n",
      " - 8s - loss: 98.1766 - val_loss: 9.0996\n",
      "Epoch 22/500\n",
      " - 8s - loss: 98.1658 - val_loss: 9.1203\n",
      "Epoch 23/500\n",
      " - 8s - loss: 98.1544 - val_loss: 9.1384\n",
      "Epoch 24/500\n",
      " - 8s - loss: 98.1441 - val_loss: 9.1692\n",
      "Epoch 25/500\n",
      " - 8s - loss: 98.1315 - val_loss: 9.1859\n",
      "Epoch 26/500\n",
      " - 7s - loss: 98.1202 - val_loss: 9.2182\n",
      "Epoch 27/500\n",
      " - 9s - loss: 98.1080 - val_loss: 9.2339\n",
      "Epoch 28/500\n",
      " - 9s - loss: 98.0959 - val_loss: 9.2674\n",
      "Epoch 29/500\n",
      " - 9s - loss: 98.0837 - val_loss: 9.2817\n",
      "Epoch 30/500\n",
      " - 8s - loss: 98.0716 - val_loss: 9.3169\n",
      "Epoch 31/500\n",
      " - 8s - loss: 98.0601 - val_loss: 9.3278\n",
      "Epoch 32/500\n",
      " - 9s - loss: 98.0478 - val_loss: 9.3649\n",
      "Epoch 33/500\n",
      " - 9s - loss: 98.0373 - val_loss: 9.3720\n",
      "Epoch 34/500\n",
      " - 9s - loss: 98.0243 - val_loss: 9.4073\n",
      "Epoch 35/500\n",
      " - 9s - loss: 98.0139 - val_loss: 9.4186\n",
      "Epoch 36/500\n",
      " - 8s - loss: 98.0027 - val_loss: 9.4388\n",
      "Epoch 37/500\n",
      " - 7s - loss: 97.9910 - val_loss: 9.4559\n",
      "Epoch 38/500\n",
      " - 7s - loss: 97.9790 - val_loss: 9.4977\n",
      "Epoch 39/500\n",
      " - 7s - loss: 97.9709 - val_loss: 9.4969\n",
      "Epoch 40/500\n",
      " - 9s - loss: 97.9576 - val_loss: 9.5205\n",
      "Epoch 41/500\n",
      " - 9s - loss: 97.9464 - val_loss: 9.5409\n",
      "Epoch 42/500\n",
      " - 8s - loss: 97.9324 - val_loss: 9.5743\n",
      "Epoch 43/500\n",
      " - 8s - loss: 97.9237 - val_loss: 9.5926\n",
      "Epoch 44/500\n",
      " - 9s - loss: 97.9145 - val_loss: 9.5480\n",
      "Epoch 45/500\n",
      " - 8s - loss: 97.8837 - val_loss: 9.6430\n",
      "Epoch 46/500\n",
      " - 8s - loss: 97.8774 - val_loss: 9.6851\n",
      "Epoch 47/500\n",
      " - 8s - loss: 97.8663 - val_loss: 9.6529\n",
      "Epoch 48/500\n",
      " - 8s - loss: 97.8372 - val_loss: 9.7238\n",
      "Epoch 49/500\n",
      " - 7s - loss: 97.8247 - val_loss: 9.7583\n",
      "Epoch 50/500\n",
      " - 8s - loss: 97.8102 - val_loss: 9.7093\n",
      "Epoch 51/500\n",
      " - 8s - loss: 97.7693 - val_loss: 9.8307\n",
      "Epoch 52/500\n",
      " - 8s - loss: 97.7623 - val_loss: 9.7925\n",
      "Epoch 53/500\n",
      " - 8s - loss: 97.7188 - val_loss: 9.9543\n",
      "Epoch 54/500\n",
      " - 8s - loss: 97.7220 - val_loss: 9.8144\n",
      "Epoch 55/500\n",
      " - 8s - loss: 97.6417 - val_loss: 9.9648\n",
      "Epoch 56/500\n",
      " - 8s - loss: 97.6171 - val_loss: 9.8543\n",
      "Epoch 57/500\n",
      " - 7s - loss: 97.5410 - val_loss: 10.0032\n",
      "Epoch 58/500\n",
      " - 7s - loss: 97.4854 - val_loss: 9.8366\n",
      "Epoch 59/500\n",
      " - 12s - loss: 97.3948 - val_loss: 9.8057\n",
      "Epoch 60/500\n",
      " - 11s - loss: 97.3165 - val_loss: 9.4718\n",
      "Epoch 61/500\n",
      " - 9s - loss: 97.2583 - val_loss: 9.9963\n",
      "Epoch 62/500\n",
      " - 8s - loss: 97.1794 - val_loss: 9.0486\n",
      "Epoch 63/500\n",
      " - 8s - loss: 97.1318 - val_loss: 11.0574\n",
      "Epoch 64/500\n",
      " - 8s - loss: 97.5584 - val_loss: 12.6007\n",
      "Epoch 65/500\n",
      " - 8s - loss: 97.5509 - val_loss: 8.9449\n",
      "Epoch 66/500\n",
      " - 7s - loss: 97.1304 - val_loss: 8.9173\n",
      "Epoch 67/500\n",
      " - 8s - loss: 96.9940 - val_loss: 8.9049\n",
      "Epoch 68/500\n",
      " - 7s - loss: 96.9354 - val_loss: 8.9103\n",
      "Epoch 69/500\n",
      " - 8s - loss: 96.8867 - val_loss: 8.9067\n",
      "Epoch 70/500\n",
      " - 8s - loss: 96.8400 - val_loss: 8.9033\n",
      "Epoch 71/500\n",
      " - 10s - loss: 96.7945 - val_loss: 8.9040\n",
      "Epoch 72/500\n",
      " - 10s - loss: 96.7541 - val_loss: 8.9003\n",
      "Epoch 73/500\n",
      " - 9s - loss: 96.7077 - val_loss: 8.8980\n",
      "Epoch 74/500\n",
      " - 7s - loss: 96.6654 - val_loss: 8.8980\n",
      "Epoch 75/500\n",
      " - 8s - loss: 96.6266 - val_loss: 8.9041\n",
      "Epoch 76/500\n",
      " - 8s - loss: 96.5920 - val_loss: 8.8872\n",
      "Epoch 77/500\n",
      " - 8s - loss: 96.9757 - val_loss: 19.2800\n",
      "Epoch 78/500\n",
      " - 7s - loss: 97.8926 - val_loss: 8.9122\n",
      "Epoch 79/500\n",
      " - 7s - loss: 96.6198 - val_loss: 8.8770\n",
      "Epoch 80/500\n",
      " - 7s - loss: 96.5540 - val_loss: 8.8803\n",
      "Epoch 81/500\n",
      " - 7s - loss: 96.5829 - val_loss: 8.8561\n",
      "Epoch 82/500\n",
      " - 7s - loss: 98.2369 - val_loss: 8.8339\n",
      "Epoch 83/500\n",
      " - 7s - loss: 97.9281 - val_loss: 8.9868\n",
      "Epoch 84/500\n",
      " - 7s - loss: 97.8751 - val_loss: 9.0155\n",
      "Epoch 85/500\n",
      " - 8s - loss: 97.8563 - val_loss: 9.0222\n",
      "Epoch 86/500\n",
      " - 7s - loss: 97.8388 - val_loss: 9.0325\n",
      "Epoch 87/500\n",
      " - 7s - loss: 97.8202 - val_loss: 9.0417\n",
      "Epoch 88/500\n",
      " - 9s - loss: 97.7984 - val_loss: 9.0563\n",
      "Epoch 89/500\n",
      " - 8s - loss: 97.7712 - val_loss: 9.0842\n",
      "Epoch 90/500\n",
      " - 9s - loss: 97.7322 - val_loss: 9.1490\n",
      "Epoch 91/500\n",
      " - 8s - loss: 97.6806 - val_loss: 9.2175\n",
      "Epoch 92/500\n",
      " - 8s - loss: 97.6573 - val_loss: 9.1897\n",
      "Epoch 93/500\n",
      " - 7s - loss: 97.6166 - val_loss: 9.1906\n",
      "Epoch 94/500\n",
      " - 8s - loss: 97.5761 - val_loss: 9.2915\n",
      "Epoch 95/500\n",
      " - 8s - loss: 97.5285 - val_loss: 9.3726\n",
      "Epoch 96/500\n",
      " - 9s - loss: 97.4861 - val_loss: 9.5626\n",
      "Epoch 97/500\n",
      " - 10s - loss: 97.3574 - val_loss: 9.7159\n",
      "Epoch 98/500\n",
      " - 9s - loss: 97.2148 - val_loss: 12.9770\n",
      "Epoch 99/500\n",
      " - 10s - loss: 96.7606 - val_loss: 9.0076\n",
      "Epoch 100/500\n",
      " - 10s - loss: 96.5448 - val_loss: 8.9744\n",
      "Epoch 101/500\n",
      " - 9s - loss: 96.4592 - val_loss: 8.9548\n",
      "Epoch 102/500\n",
      " - 9s - loss: 96.3966 - val_loss: 8.9498\n",
      "Epoch 103/500\n",
      " - 9s - loss: 96.3554 - val_loss: 8.9314\n",
      "Epoch 104/500\n",
      " - 9s - loss: 96.3484 - val_loss: 8.9403\n",
      "Epoch 105/500\n",
      " - 9s - loss: 96.3300 - val_loss: 8.9216\n",
      "Epoch 106/500\n",
      " - 9s - loss: 96.2537 - val_loss: 8.9918\n",
      "Epoch 107/500\n",
      " - 8s - loss: 96.3047 - val_loss: 8.9075\n",
      "Epoch 108/500\n",
      " - 9s - loss: 96.2039 - val_loss: 8.9057\n",
      "Epoch 109/500\n",
      " - 10s - loss: 97.3478 - val_loss: 20.6429\n",
      "Epoch 110/500\n",
      " - 9s - loss: 98.1578 - val_loss: 8.9348\n",
      "Epoch 111/500\n",
      " - 9s - loss: 96.3519 - val_loss: 8.9297\n",
      "Epoch 112/500\n",
      " - 9s - loss: 96.2844 - val_loss: 8.9232\n",
      "Epoch 113/500\n",
      " - 9s - loss: 96.2276 - val_loss: 8.9234\n",
      "Epoch 114/500\n",
      " - 9s - loss: 96.1932 - val_loss: 9.0597\n",
      "Epoch 115/500\n",
      " - 7s - loss: 96.2939 - val_loss: 8.9185\n",
      "Epoch 116/500\n",
      " - 8s - loss: 96.2000 - val_loss: 8.9102\n",
      "Epoch 117/500\n",
      " - 8s - loss: 96.1326 - val_loss: 8.9115\n",
      "Epoch 118/500\n",
      " - 7s - loss: 96.6475 - val_loss: 17.6690\n",
      "Epoch 119/500\n",
      " - 8s - loss: 96.3947 - val_loss: 8.8939\n",
      "Epoch 120/500\n",
      " - 8s - loss: 96.1136 - val_loss: 8.9035\n",
      "Epoch 121/500\n",
      " - 9s - loss: 96.0606 - val_loss: 8.8997\n",
      "Epoch 122/500\n",
      " - 9s - loss: 96.8532 - val_loss: 23.5373\n",
      "Epoch 123/500\n",
      " - 9s - loss: 98.5812 - val_loss: 8.8894\n",
      "Epoch 124/500\n",
      " - 9s - loss: 96.1398 - val_loss: 8.8967\n",
      "Epoch 125/500\n",
      " - 9s - loss: 96.0888 - val_loss: 8.8977\n",
      "Epoch 126/500\n",
      " - 9s - loss: 96.0594 - val_loss: 8.9032\n",
      "Epoch 127/500\n",
      " - 10s - loss: 96.0375 - val_loss: 8.8901\n",
      "Epoch 128/500\n",
      " - 8s - loss: 97.7864 - val_loss: 21.1203\n",
      "Epoch 129/500\n",
      " - 7s - loss: 98.2097 - val_loss: 8.9271\n",
      "Epoch 130/500\n",
      " - 8s - loss: 96.2041 - val_loss: 8.9302\n",
      "Epoch 131/500\n",
      " - 8s - loss: 96.1567 - val_loss: 8.9288\n",
      "Epoch 132/500\n",
      " - 8s - loss: 96.1139 - val_loss: 8.9283\n",
      "Epoch 133/500\n",
      " - 8s - loss: 96.0777 - val_loss: 8.9279\n",
      "Epoch 134/500\n",
      " - 8s - loss: 96.0503 - val_loss: 8.9362\n",
      "Epoch 135/500\n",
      " - 8s - loss: 96.0321 - val_loss: 8.9301\n",
      "Epoch 136/500\n",
      " - 7s - loss: 96.0130 - val_loss: 9.0012\n",
      "Epoch 137/500\n",
      " - 8s - loss: 96.0652 - val_loss: 8.9270\n",
      "Epoch 138/500\n",
      " - 8s - loss: 96.0034 - val_loss: 8.9259\n",
      "Epoch 139/500\n",
      " - 9s - loss: 95.9704 - val_loss: 8.9242\n",
      "Epoch 140/500\n",
      " - 9s - loss: 96.0083 - val_loss: 9.4016\n",
      "Epoch 141/500\n",
      " - 9s - loss: 96.2412 - val_loss: 8.9079\n",
      "Epoch 142/500\n",
      " - 9s - loss: 95.9441 - val_loss: 8.9229\n",
      "Epoch 143/500\n",
      " - 9s - loss: 95.9214 - val_loss: 8.9122\n",
      "Epoch 144/500\n",
      " - 9s - loss: 95.9317 - val_loss: 9.2012\n",
      "Epoch 145/500\n",
      " - 9s - loss: 96.1840 - val_loss: 8.9214\n",
      "Epoch 146/500\n",
      " - 9s - loss: 96.0577 - val_loss: 8.9073\n",
      "Epoch 147/500\n",
      " - 8s - loss: 95.9697 - val_loss: 8.9003\n",
      "Epoch 148/500\n",
      " - 8s - loss: 95.8931 - val_loss: 8.9100\n",
      "Epoch 149/500\n",
      " - 8s - loss: 95.8748 - val_loss: 8.8978\n",
      "Epoch 150/500\n",
      " - 9s - loss: 95.8379 - val_loss: 8.8879\n",
      "Epoch 151/500\n",
      " - 9s - loss: 101.1317 - val_loss: 38.1501\n",
      "Epoch 152/500\n",
      " - 9s - loss: 112.9129 - val_loss: 16.0437\n",
      "Epoch 153/500\n",
      " - 8s - loss: 101.7202 - val_loss: 8.9220\n",
      "Epoch 154/500\n",
      " - 8s - loss: 97.0004 - val_loss: 8.9281\n",
      "Epoch 155/500\n",
      " - 8s - loss: 96.5272 - val_loss: 8.8848\n",
      "Epoch 156/500\n",
      " - 8s - loss: 96.4452 - val_loss: 8.9011\n",
      "Epoch 157/500\n",
      " - 8s - loss: 96.4116 - val_loss: 8.9084\n",
      "Epoch 158/500\n",
      " - 9s - loss: 96.3817 - val_loss: 8.9115\n",
      "Epoch 159/500\n",
      " - 9s - loss: 96.3522 - val_loss: 8.9146\n",
      "Epoch 160/500\n",
      " - 8s - loss: 96.3240 - val_loss: 8.9161\n",
      "Epoch 161/500\n",
      " - 8s - loss: 96.2964 - val_loss: 8.9181\n",
      "Epoch 162/500\n",
      " - 8s - loss: 96.2692 - val_loss: 8.9200\n",
      "Epoch 163/500\n",
      " - 8s - loss: 96.2428 - val_loss: 8.9209\n",
      "Epoch 164/500\n",
      " - 8s - loss: 96.2174 - val_loss: 8.9208\n",
      "Epoch 165/500\n",
      " - 8s - loss: 96.1914 - val_loss: 8.9249\n",
      "Epoch 166/500\n",
      " - 7s - loss: 96.1691 - val_loss: 8.9232\n",
      "Epoch 167/500\n",
      " - 8s - loss: 96.1450 - val_loss: 8.9259\n",
      "Epoch 168/500\n",
      " - 9s - loss: 96.1233 - val_loss: 8.9261\n",
      "Epoch 169/500\n",
      " - 10s - loss: 96.1027 - val_loss: 8.9278\n",
      "Epoch 170/500\n",
      " - 7s - loss: 96.0825 - val_loss: 8.9273\n",
      "Epoch 171/500\n",
      " - 7s - loss: 96.0630 - val_loss: 8.9283\n",
      "Epoch 172/500\n",
      " - 8s - loss: 96.0432 - val_loss: 8.9296\n",
      "Epoch 173/500\n",
      " - 8s - loss: 96.0260 - val_loss: 8.9256\n",
      "Epoch 174/500\n",
      " - 8s - loss: 96.0046 - val_loss: 8.9279\n",
      "Epoch 175/500\n",
      " - 8s - loss: 95.9869 - val_loss: 8.9222\n",
      "Epoch 176/500\n",
      " - 8s - loss: 95.9689 - val_loss: 8.9513\n",
      "Epoch 177/500\n",
      " - 9s - loss: 95.9629 - val_loss: 8.9305\n",
      "Epoch 178/500\n",
      " - 9s - loss: 95.9440 - val_loss: 8.9302\n",
      "Epoch 179/500\n",
      " - 9s - loss: 95.9298 - val_loss: 8.9250\n",
      "Epoch 180/500\n",
      " - 9s - loss: 95.9111 - val_loss: 8.9154\n",
      "Epoch 181/500\n",
      " - 8s - loss: 95.9082 - val_loss: 9.0229\n",
      "Epoch 182/500\n",
      " - 7s - loss: 95.9800 - val_loss: 8.9212\n",
      "Epoch 183/500\n",
      " - 7s - loss: 95.9095 - val_loss: 8.9241\n",
      "Epoch 184/500\n",
      " - 8s - loss: 95.8921 - val_loss: 8.9208\n",
      "Epoch 185/500\n",
      " - 8s - loss: 95.8617 - val_loss: 8.9159\n",
      "Epoch 186/500\n",
      " - 8s - loss: 95.8431 - val_loss: 8.9230\n",
      "Epoch 187/500\n",
      " - 8s - loss: 95.8302 - val_loss: 8.9214\n",
      "Epoch 188/500\n",
      " - 8s - loss: 95.8196 - val_loss: 8.9158\n",
      "Epoch 189/500\n",
      " - 8s - loss: 95.8035 - val_loss: 8.9004\n",
      "Epoch 190/500\n",
      " - 8s - loss: 97.4807 - val_loss: 41.3334\n",
      "Epoch 191/500\n",
      " - 7s - loss: 116.9266 - val_loss: 14.9903\n",
      "Epoch 192/500\n",
      " - 8s - loss: 96.5527 - val_loss: 8.9034\n",
      "Epoch 193/500\n",
      " - 8s - loss: 96.3311 - val_loss: 8.8795\n",
      "Epoch 194/500\n",
      " - 8s - loss: 96.2809 - val_loss: 8.8844\n",
      "Epoch 195/500\n",
      " - 8s - loss: 96.2500 - val_loss: 8.8857\n",
      "Epoch 196/500\n",
      " - 8s - loss: 96.2201 - val_loss: 8.8875\n",
      "Epoch 197/500\n",
      " - 8s - loss: 96.1960 - val_loss: 8.8865\n",
      "Epoch 198/500\n",
      " - 7s - loss: 96.1677 - val_loss: 8.8903\n",
      "Epoch 199/500\n",
      " - 7s - loss: 96.1459 - val_loss: 8.8937\n",
      "Epoch 200/500\n",
      " - 8s - loss: 96.1276 - val_loss: 8.8940\n",
      "Epoch 201/500\n",
      " - 8s - loss: 96.1095 - val_loss: 8.8945\n",
      "Epoch 202/500\n",
      " - 8s - loss: 96.0916 - val_loss: 8.8921\n",
      "Epoch 203/500\n",
      " - 9s - loss: 96.0661 - val_loss: 8.8890\n",
      "Epoch 204/500\n",
      " - 8s - loss: 96.0433 - val_loss: 8.8962\n",
      "Epoch 205/500\n",
      " - 8s - loss: 96.0254 - val_loss: 8.8959\n",
      "Epoch 206/500\n",
      " - 9s - loss: 96.0092 - val_loss: 8.8977\n",
      "Epoch 207/500\n",
      " - 9s - loss: 95.9919 - val_loss: 8.8972\n",
      "Epoch 208/500\n",
      " - 9s - loss: 95.9762 - val_loss: 8.8986\n",
      "Epoch 209/500\n",
      " - 9s - loss: 95.9596 - val_loss: 8.8972\n",
      "Epoch 210/500\n",
      " - 9s - loss: 95.9441 - val_loss: 8.8994\n",
      "Epoch 211/500\n",
      " - 9s - loss: 95.9287 - val_loss: 8.8992\n",
      "Epoch 212/500\n",
      " - 9s - loss: 95.9138 - val_loss: 8.8983\n",
      "Epoch 213/500\n",
      " - 8s - loss: 95.8987 - val_loss: 8.8972\n",
      "Epoch 214/500\n",
      " - 8s - loss: 95.8840 - val_loss: 8.8949\n",
      "Epoch 215/500\n",
      " - 7s - loss: 95.8692 - val_loss: 8.8932\n",
      "Epoch 216/500\n",
      " - 10s - loss: 95.8550 - val_loss: 8.8996\n",
      "Epoch 217/500\n",
      " - 10s - loss: 95.8436 - val_loss: 8.8969\n",
      "Epoch 218/500\n",
      " - 9s - loss: 95.8317 - val_loss: 8.8831\n",
      "Epoch 219/500\n",
      " - 8s - loss: 95.8051 - val_loss: 8.9262\n",
      "Epoch 220/500\n",
      " - 8s - loss: 95.8096 - val_loss: 8.8884\n",
      "Epoch 221/500\n",
      " - 8s - loss: 95.7940 - val_loss: 8.8925\n",
      "Epoch 222/500\n",
      " - 8s - loss: 95.7811 - val_loss: 8.8932\n",
      "Epoch 223/500\n",
      " - 9s - loss: 95.7674 - val_loss: 8.8873\n",
      "Epoch 224/500\n",
      " - 8s - loss: 95.7545 - val_loss: 8.8867\n",
      "Epoch 225/500\n",
      " - 8s - loss: 95.7429 - val_loss: 8.8856\n",
      "Epoch 226/500\n",
      " - 9s - loss: 95.7311 - val_loss: 8.8774\n",
      "Epoch 227/500\n",
      " - 9s - loss: 95.7253 - val_loss: 8.8654\n",
      "Epoch 228/500\n",
      " - 8s - loss: 96.3453 - val_loss: 9.0020\n",
      "Epoch 229/500\n",
      " - 8s - loss: 95.8300 - val_loss: 8.9017\n",
      "Epoch 230/500\n",
      " - 8s - loss: 95.7602 - val_loss: 8.9012\n",
      "Epoch 231/500\n",
      " - 8s - loss: 95.7422 - val_loss: 8.8931\n",
      "Epoch 232/500\n",
      " - 8s - loss: 95.7096 - val_loss: 8.8677\n",
      "Epoch 233/500\n",
      " - 8s - loss: 99.1350 - val_loss: 9.9155\n",
      "Epoch 234/500\n",
      " - 8s - loss: 96.1642 - val_loss: 8.8947\n",
      "Epoch 235/500\n",
      " - 8s - loss: 95.8473 - val_loss: 8.8824\n",
      "Epoch 236/500\n",
      " - 8s - loss: 95.7448 - val_loss: 8.8775\n",
      "Epoch 237/500\n",
      " - 8s - loss: 95.7004 - val_loss: 8.8744\n",
      "Epoch 238/500\n",
      " - 8s - loss: 95.6761 - val_loss: 8.8703\n",
      "Epoch 239/500\n",
      " - 8s - loss: 97.2252 - val_loss: 42.0260\n",
      "Epoch 240/500\n",
      " - 8s - loss: 113.7740 - val_loss: 9.0356\n",
      "Epoch 241/500\n",
      " - 8s - loss: 96.2372 - val_loss: 8.8678\n",
      "Epoch 242/500\n",
      " - 8s - loss: 96.1392 - val_loss: 8.8644\n",
      "Epoch 243/500\n",
      " - 8s - loss: 96.0941 - val_loss: 8.8634\n",
      "Epoch 244/500\n",
      " - 8s - loss: 96.0553 - val_loss: 8.8611\n",
      "Epoch 245/500\n",
      " - 8s - loss: 96.0230 - val_loss: 8.8559\n",
      "Epoch 246/500\n",
      " - 8s - loss: 95.9857 - val_loss: 8.8693\n",
      "Epoch 247/500\n",
      " - 7s - loss: 95.9639 - val_loss: 8.8715\n",
      "Epoch 248/500\n",
      " - 7s - loss: 95.9456 - val_loss: 8.8682\n",
      "Epoch 249/500\n",
      " - 8s - loss: 95.9314 - val_loss: 8.8611\n",
      "Epoch 250/500\n",
      " - 7s - loss: 95.8967 - val_loss: 8.8696\n",
      "Epoch 251/500\n",
      " - 7s - loss: 95.8760 - val_loss: 8.8691\n",
      "Epoch 252/500\n",
      " - 8s - loss: 95.8605 - val_loss: 8.8688\n",
      "Epoch 253/500\n",
      " - 8s - loss: 95.8448 - val_loss: 8.8702\n",
      "Epoch 254/500\n",
      " - 9s - loss: 95.8312 - val_loss: 8.8681\n",
      "Epoch 255/500\n",
      " - 9s - loss: 95.8172 - val_loss: 8.8668\n",
      "Epoch 256/500\n",
      " - 9s - loss: 95.8034 - val_loss: 8.8689\n",
      "Epoch 257/500\n",
      " - 9s - loss: 95.7901 - val_loss: 8.8700\n",
      "Epoch 258/500\n",
      " - 9s - loss: 95.7777 - val_loss: 8.8730\n",
      "Epoch 259/500\n",
      " - 9s - loss: 95.7661 - val_loss: 8.8696\n",
      "Epoch 260/500\n",
      " - 9s - loss: 95.7548 - val_loss: 8.8680\n",
      "Epoch 261/500\n",
      " - 9s - loss: 95.7438 - val_loss: 8.8709\n",
      "Epoch 262/500\n",
      " - 9s - loss: 95.7318 - val_loss: 8.8698\n",
      "Epoch 263/500\n",
      " - 9s - loss: 95.7206 - val_loss: 8.8676\n",
      "Epoch 264/500\n",
      " - 10s - loss: 95.7088 - val_loss: 8.8655\n",
      "Epoch 265/500\n",
      " - 10s - loss: 95.6967 - val_loss: 8.8621\n",
      "Epoch 266/500\n",
      " - 8s - loss: 95.6842 - val_loss: 8.8632\n",
      "Epoch 267/500\n",
      " - 8s - loss: 95.6726 - val_loss: 8.8623\n",
      "Epoch 268/500\n",
      " - 9s - loss: 95.6652 - val_loss: 8.8637\n",
      "Epoch 269/500\n",
      " - 9s - loss: 95.6542 - val_loss: 8.8594\n",
      "Epoch 270/500\n",
      " - 9s - loss: 95.6432 - val_loss: 8.8507\n",
      "Epoch 271/500\n",
      " - 9s - loss: 95.6437 - val_loss: 8.8341\n",
      "Epoch 272/500\n",
      " - 9s - loss: 97.5594 - val_loss: 9.0303\n",
      "Epoch 273/500\n",
      " - 9s - loss: 97.5282 - val_loss: 9.0659\n",
      "Epoch 274/500\n",
      " - 9s - loss: 98.7649 - val_loss: 8.9034\n",
      "Epoch 275/500\n",
      " - 10s - loss: 97.4022 - val_loss: 9.0454\n",
      "Epoch 276/500\n",
      " - 9s - loss: 97.3350 - val_loss: 9.1155\n",
      "Epoch 277/500\n",
      " - 7s - loss: 97.2012 - val_loss: 9.0390\n",
      "Epoch 278/500\n",
      " - 8s - loss: 97.0818 - val_loss: 9.1425\n",
      "Epoch 279/500\n",
      " - 7s - loss: 97.1167 - val_loss: 8.9403\n",
      "Epoch 280/500\n",
      " - 7s - loss: 97.1120 - val_loss: 9.0472\n",
      "Epoch 281/500\n",
      " - 7s - loss: 96.9605 - val_loss: 9.0663\n",
      "Epoch 282/500\n",
      " - 7s - loss: 96.9262 - val_loss: 9.0672\n",
      "Epoch 283/500\n",
      " - 8s - loss: 96.8717 - val_loss: 9.1008\n",
      "Epoch 284/500\n",
      " - 10s - loss: 96.7948 - val_loss: 9.1997\n",
      "Epoch 285/500\n",
      " - 9s - loss: 96.8017 - val_loss: 9.0534\n",
      "Epoch 286/500\n",
      " - 10s - loss: 96.6944 - val_loss: 9.1476\n",
      "Epoch 287/500\n",
      " - 10s - loss: 96.6804 - val_loss: 8.9619\n",
      "Epoch 288/500\n",
      " - 9s - loss: 96.7911 - val_loss: 9.3247\n",
      "Epoch 289/500\n",
      " - 10s - loss: 96.6665 - val_loss: 9.1158\n",
      "Epoch 290/500\n",
      " - 9s - loss: 96.4895 - val_loss: 9.0147\n",
      "Epoch 291/500\n",
      " - 9s - loss: 96.3559 - val_loss: 9.0457\n",
      "Epoch 292/500\n",
      " - 9s - loss: 96.2669 - val_loss: 9.0485\n",
      "Epoch 293/500\n",
      " - 8s - loss: 96.0976 - val_loss: 8.9603\n",
      "Epoch 294/500\n",
      " - 8s - loss: 95.9696 - val_loss: 8.9224\n",
      "Epoch 295/500\n",
      " - 9s - loss: 95.8524 - val_loss: 8.9019\n",
      "Epoch 296/500\n",
      " - 9s - loss: 95.7607 - val_loss: 8.8903\n",
      "Epoch 297/500\n",
      " - 9s - loss: 95.6900 - val_loss: 8.8817\n",
      "Epoch 298/500\n",
      " - 9s - loss: 95.6421 - val_loss: 8.8931\n",
      "Epoch 299/500\n",
      " - 9s - loss: 95.6120 - val_loss: 8.8773\n",
      "Epoch 300/500\n",
      " - 8s - loss: 95.5735 - val_loss: 8.8854\n",
      "Epoch 301/500\n",
      " - 8s - loss: 95.5695 - val_loss: 8.8667\n",
      "Epoch 302/500\n",
      " - 8s - loss: 95.9961 - val_loss: 8.9993\n",
      "Epoch 303/500\n",
      " - 9s - loss: 95.8883 - val_loss: 8.9302\n",
      "Epoch 304/500\n",
      " - 11s - loss: 95.7554 - val_loss: 8.8905\n",
      "Epoch 305/500\n",
      " - 9s - loss: 95.5891 - val_loss: 8.9005\n",
      "Epoch 306/500\n",
      " - 8s - loss: 95.5521 - val_loss: 8.8833\n",
      "Epoch 307/500\n",
      " - 9s - loss: 95.5050 - val_loss: 8.8891\n",
      "Epoch 308/500\n",
      " - 9s - loss: 95.5594 - val_loss: 8.8706\n",
      "Epoch 309/500\n",
      " - 8s - loss: 95.4777 - val_loss: 8.8747\n",
      "Epoch 310/500\n",
      " - 8s - loss: 95.4667 - val_loss: 8.8685\n",
      "Epoch 311/500\n",
      " - 9s - loss: 95.4578 - val_loss: 8.8705\n",
      "Epoch 312/500\n",
      " - 10s - loss: 95.4732 - val_loss: 8.8640\n",
      "Epoch 313/500\n",
      " - 9s - loss: 97.4310 - val_loss: 9.0494\n",
      "Epoch 314/500\n",
      " - 8s - loss: 99.0952 - val_loss: 8.9836\n",
      "Epoch 315/500\n",
      " - 9s - loss: 97.6255 - val_loss: 9.0839\n",
      "Epoch 316/500\n",
      " - 8s - loss: 97.5601 - val_loss: 9.0130\n",
      "Epoch 317/500\n",
      " - 8s - loss: 97.5097 - val_loss: 8.9275\n",
      "Epoch 318/500\n",
      " - 8s - loss: 97.4168 - val_loss: 8.9098\n",
      "Epoch 319/500\n",
      " - 8s - loss: 97.3935 - val_loss: 8.9795\n",
      "Epoch 320/500\n",
      " - 8s - loss: 97.3346 - val_loss: 9.0091\n",
      "Epoch 321/500\n",
      " - 9s - loss: 97.2823 - val_loss: 9.0749\n",
      "Epoch 322/500\n",
      " - 10s - loss: 97.2133 - val_loss: 9.0796\n",
      "Epoch 323/500\n",
      " - 8s - loss: 97.1987 - val_loss: 9.0535\n",
      "Epoch 324/500\n",
      " - 7s - loss: 97.1565 - val_loss: 9.1279\n",
      "Epoch 325/500\n",
      " - 8s - loss: 97.1462 - val_loss: 9.0925\n",
      "Epoch 326/500\n",
      " - 8s - loss: 97.0675 - val_loss: 9.0653\n",
      "Epoch 327/500\n",
      " - 9s - loss: 97.0304 - val_loss: 9.0669\n",
      "Epoch 328/500\n",
      " - 8s - loss: 97.0021 - val_loss: 9.0572\n",
      "Epoch 329/500\n",
      " - 9s - loss: 96.9713 - val_loss: 9.0748\n",
      "Epoch 330/500\n",
      " - 9s - loss: 96.9491 - val_loss: 9.0670\n",
      "Epoch 331/500\n",
      " - 9s - loss: 96.9262 - val_loss: 9.0726\n",
      "Epoch 332/500\n",
      " - 8s - loss: 96.8964 - val_loss: 9.0907\n",
      "Epoch 333/500\n",
      " - 8s - loss: 96.8731 - val_loss: 9.1008\n",
      "Epoch 334/500\n",
      " - 9s - loss: 96.8523 - val_loss: 9.0880\n",
      "Epoch 335/500\n",
      " - 9s - loss: 96.8225 - val_loss: 9.0881\n",
      "Epoch 336/500\n",
      " - 9s - loss: 96.7919 - val_loss: 9.1268\n",
      "Epoch 337/500\n",
      " - 9s - loss: 96.7853 - val_loss: 9.1129\n",
      "Epoch 338/500\n",
      " - 9s - loss: 96.7611 - val_loss: 9.1093\n",
      "Epoch 339/500\n",
      " - 9s - loss: 96.7312 - val_loss: 9.1049\n",
      "Epoch 340/500\n",
      " - 8s - loss: 96.7108 - val_loss: 9.1250\n",
      "Epoch 341/500\n",
      " - 7s - loss: 96.6936 - val_loss: 9.1387\n",
      "Epoch 342/500\n",
      " - 8s - loss: 96.6173 - val_loss: 9.1363\n",
      "Epoch 343/500\n",
      " - 9s - loss: 96.5945 - val_loss: 9.1244\n",
      "Epoch 344/500\n",
      " - 9s - loss: 96.5401 - val_loss: 9.1530\n",
      "Epoch 345/500\n",
      " - 10s - loss: 96.5075 - val_loss: 9.1374\n",
      "Epoch 346/500\n",
      " - 10s - loss: 96.5035 - val_loss: 9.1115\n",
      "Epoch 347/500\n",
      " - 9s - loss: 96.4316 - val_loss: 9.0512\n",
      "Epoch 348/500\n",
      " - 9s - loss: 96.3951 - val_loss: 9.0784\n",
      "Epoch 349/500\n",
      " - 9s - loss: 96.4199 - val_loss: 9.0750\n",
      "Epoch 350/500\n",
      " - 9s - loss: 96.1817 - val_loss: 9.0503\n",
      "Epoch 351/500\n",
      " - 9s - loss: 96.1182 - val_loss: 9.0303\n",
      "Epoch 352/500\n",
      " - 9s - loss: 96.0336 - val_loss: 9.0232\n",
      "Epoch 353/500\n",
      " - 9s - loss: 96.0157 - val_loss: 9.0105\n",
      "Epoch 354/500\n",
      " - 8s - loss: 95.9225 - val_loss: 9.0184\n",
      "Epoch 355/500\n",
      " - 9s - loss: 95.8530 - val_loss: 9.0179\n",
      "Epoch 356/500\n",
      " - 9s - loss: 95.7554 - val_loss: 9.0180\n",
      "Epoch 357/500\n",
      " - 9s - loss: 95.6814 - val_loss: 9.0086\n",
      "Epoch 358/500\n",
      " - 9s - loss: 95.6219 - val_loss: 9.0048\n",
      "Epoch 359/500\n",
      " - 9s - loss: 95.5693 - val_loss: 8.9953\n",
      "Epoch 360/500\n",
      " - 9s - loss: 95.5404 - val_loss: 8.9639\n",
      "Epoch 361/500\n",
      " - 9s - loss: 102.0409 - val_loss: 55.0854\n",
      "Epoch 362/500\n",
      " - 9s - loss: 117.9741 - val_loss: 16.6874\n",
      "Epoch 363/500\n",
      " - 9s - loss: 97.5248 - val_loss: 9.2247\n",
      "Epoch 364/500\n",
      " - 8s - loss: 96.3887 - val_loss: 9.2135\n",
      "Epoch 365/500\n",
      " - 8s - loss: 96.3355 - val_loss: 9.1974\n",
      "Epoch 366/500\n",
      " - 10s - loss: 96.2955 - val_loss: 9.1812\n",
      "Epoch 367/500\n",
      " - 8s - loss: 96.2602 - val_loss: 9.1676\n",
      "Epoch 368/500\n",
      " - 8s - loss: 96.2276 - val_loss: 9.1593\n",
      "Epoch 369/500\n",
      " - 8s - loss: 96.1916 - val_loss: 9.1262\n",
      "Epoch 370/500\n",
      " - 8s - loss: 96.1475 - val_loss: 9.1131\n",
      "Epoch 371/500\n",
      " - 8s - loss: 96.1147 - val_loss: 9.0931\n",
      "Epoch 372/500\n",
      " - 9s - loss: 96.0657 - val_loss: 9.0753\n",
      "Epoch 373/500\n",
      " - 10s - loss: 96.0004 - val_loss: 9.0406\n",
      "Epoch 374/500\n",
      " - 9s - loss: 95.9443 - val_loss: 9.0003\n",
      "Epoch 375/500\n",
      " - 7s - loss: 95.9076 - val_loss: 8.9721\n",
      "Epoch 376/500\n",
      " - 7s - loss: 95.8737 - val_loss: 8.9705\n",
      "Epoch 377/500\n",
      " - 9s - loss: 95.8486 - val_loss: 8.9587\n",
      "Epoch 378/500\n",
      " - 8s - loss: 95.8147 - val_loss: 8.9680\n",
      "Epoch 379/500\n",
      " - 9s - loss: 95.7906 - val_loss: 8.9557\n",
      "Epoch 380/500\n",
      " - 9s - loss: 95.7605 - val_loss: 8.9517\n",
      "Epoch 381/500\n",
      " - 8s - loss: 95.7430 - val_loss: 8.9467\n",
      "Epoch 382/500\n",
      " - 8s - loss: 95.7123 - val_loss: 8.9439\n",
      "Epoch 383/500\n",
      " - 7s - loss: 95.6819 - val_loss: 8.9601\n",
      "Epoch 384/500\n",
      " - 10s - loss: 95.6706 - val_loss: 8.9477\n",
      "Epoch 385/500\n",
      " - 12s - loss: 95.6431 - val_loss: 8.9441\n",
      "Epoch 386/500\n",
      " - 9s - loss: 95.6232 - val_loss: 8.9416\n",
      "Epoch 387/500\n",
      " - 9s - loss: 95.6033 - val_loss: 8.9373\n",
      "Epoch 388/500\n",
      " - 8s - loss: 95.5859 - val_loss: 8.9421\n",
      "Epoch 389/500\n",
      " - 10s - loss: 95.5637 - val_loss: 8.9297\n",
      "Epoch 390/500\n",
      " - 9s - loss: 95.5286 - val_loss: 8.9427\n",
      "Epoch 391/500\n",
      " - 9s - loss: 95.5145 - val_loss: 8.9329\n",
      "Epoch 392/500\n",
      " - 9s - loss: 95.4996 - val_loss: 8.9338\n",
      "Epoch 393/500\n",
      " - 10s - loss: 95.4795 - val_loss: 8.9420\n",
      "Epoch 394/500\n",
      " - 9s - loss: 95.4664 - val_loss: 8.9313\n",
      "Epoch 395/500\n",
      " - 9s - loss: 95.4463 - val_loss: 8.9393\n",
      "Epoch 396/500\n",
      " - 10s - loss: 95.4339 - val_loss: 8.9280\n",
      "Epoch 397/500\n",
      " - 9s - loss: 95.4153 - val_loss: 8.9393\n",
      "Epoch 398/500\n",
      " - 8s - loss: 95.4193 - val_loss: 8.9246\n",
      "Epoch 399/500\n",
      " - 8s - loss: 95.7565 - val_loss: 9.2449\n",
      "Epoch 400/500\n",
      " - 7s - loss: 100.3815 - val_loss: 9.0138\n",
      "Epoch 401/500\n",
      " - 7s - loss: 95.5548 - val_loss: 8.8984\n",
      "Epoch 402/500\n",
      " - 8s - loss: 95.4956 - val_loss: 8.8868\n",
      "Epoch 403/500\n",
      " - 8s - loss: 95.4570 - val_loss: 8.8928\n",
      "Epoch 404/500\n",
      " - 7s - loss: 95.4191 - val_loss: 8.9168\n",
      "Epoch 405/500\n",
      " - 7s - loss: 95.4144 - val_loss: 8.8981\n",
      "Epoch 406/500\n",
      " - 7s - loss: 95.3962 - val_loss: 8.9154\n",
      "Epoch 407/500\n",
      " - 7s - loss: 95.3928 - val_loss: 8.9064\n",
      "Epoch 408/500\n",
      " - 8s - loss: 95.3754 - val_loss: 8.9141\n",
      "Epoch 409/500\n",
      " - 8s - loss: 95.3695 - val_loss: 8.9118\n",
      "Epoch 410/500\n",
      " - 8s - loss: 95.3586 - val_loss: 8.9205\n",
      "Epoch 411/500\n",
      " - 8s - loss: 95.3525 - val_loss: 8.9199\n",
      "Epoch 412/500\n",
      " - 7s - loss: 95.3437 - val_loss: 8.9147\n",
      "Epoch 413/500\n",
      " - 8s - loss: 95.3341 - val_loss: 8.9259\n",
      "Epoch 414/500\n",
      " - 8s - loss: 95.3297 - val_loss: 8.9238\n",
      "Epoch 415/500\n",
      " - 8s - loss: 95.3204 - val_loss: 8.9230\n",
      "Epoch 416/500\n",
      " - 7s - loss: 95.3141 - val_loss: 8.9281\n",
      "Epoch 417/500\n",
      " - 7s - loss: 95.3077 - val_loss: 8.9306\n",
      "Epoch 418/500\n",
      " - 8s - loss: 95.3006 - val_loss: 8.9324\n",
      "Epoch 419/500\n",
      " - 8s - loss: 95.2937 - val_loss: 8.9284\n",
      "Epoch 420/500\n",
      " - 7s - loss: 95.2873 - val_loss: 8.9348\n",
      "Epoch 421/500\n",
      " - 8s - loss: 95.2833 - val_loss: 8.9347\n",
      "Epoch 422/500\n",
      " - 7s - loss: 95.2773 - val_loss: 8.9386\n",
      "Epoch 423/500\n",
      " - 8s - loss: 95.2727 - val_loss: 8.9320\n",
      "Epoch 424/500\n",
      " - 8s - loss: 95.2669 - val_loss: 8.9349\n",
      "Epoch 425/500\n",
      " - 8s - loss: 95.2634 - val_loss: 8.9355\n",
      "Epoch 426/500\n",
      " - 8s - loss: 95.2594 - val_loss: 8.9407\n",
      "Epoch 427/500\n",
      " - 8s - loss: 95.2552 - val_loss: 8.9364\n",
      "Epoch 428/500\n",
      " - 8s - loss: 95.2500 - val_loss: 8.9417\n",
      "Epoch 429/500\n",
      " - 8s - loss: 95.2472 - val_loss: 8.9355\n",
      "Epoch 430/500\n",
      " - 9s - loss: 95.2430 - val_loss: 8.9356\n",
      "Epoch 431/500\n",
      " - 9s - loss: 95.2387 - val_loss: 8.9344\n",
      "Epoch 432/500\n",
      " - 9s - loss: 95.2348 - val_loss: 8.9438\n",
      "Epoch 433/500\n",
      " - 8s - loss: 95.2330 - val_loss: 8.9434\n",
      "Epoch 434/500\n",
      " - 8s - loss: 95.2282 - val_loss: 8.9490\n",
      "Epoch 435/500\n",
      " - 9s - loss: 95.2273 - val_loss: 8.9474\n",
      "Epoch 436/500\n",
      " - 9s - loss: 95.2223 - val_loss: 8.9453\n",
      "Epoch 437/500\n",
      " - 9s - loss: 95.2175 - val_loss: 8.9456\n",
      "Epoch 438/500\n",
      " - 9s - loss: 95.2144 - val_loss: 8.9510\n",
      "Epoch 439/500\n",
      " - 8s - loss: 95.2115 - val_loss: 8.9497\n",
      "Epoch 440/500\n",
      " - 7s - loss: 95.2077 - val_loss: 8.9458\n",
      "Epoch 441/500\n",
      " - 8s - loss: 95.2035 - val_loss: 8.9555\n",
      "Epoch 442/500\n",
      " - 8s - loss: 95.2029 - val_loss: 8.9517\n",
      "Epoch 443/500\n",
      " - 8s - loss: 95.1978 - val_loss: 8.9506\n",
      "Epoch 444/500\n",
      " - 8s - loss: 95.1945 - val_loss: 8.9479\n",
      "Epoch 445/500\n",
      " - 9s - loss: 95.1923 - val_loss: 8.9563\n",
      "Epoch 446/500\n",
      " - 9s - loss: 95.1918 - val_loss: 8.9539\n",
      "Epoch 447/500\n",
      " - 8s - loss: 95.1864 - val_loss: 8.9618\n",
      "Epoch 448/500\n",
      " - 8s - loss: 95.1850 - val_loss: 8.9593\n",
      "Epoch 449/500\n",
      " - 9s - loss: 95.1805 - val_loss: 8.9630\n",
      "Epoch 450/500\n",
      " - 9s - loss: 95.1778 - val_loss: 8.9564\n",
      "Epoch 451/500\n",
      " - 7s - loss: 95.1737 - val_loss: 8.9656\n",
      "Epoch 452/500\n",
      " - 7s - loss: 95.1741 - val_loss: 8.9561\n",
      "Epoch 453/500\n",
      " - 8s - loss: 95.1697 - val_loss: 8.9599\n",
      "Epoch 454/500\n",
      " - 8s - loss: 95.1670 - val_loss: 8.9654\n",
      "Epoch 455/500\n",
      " - 9s - loss: 95.1646 - val_loss: 8.9616\n",
      "Epoch 456/500\n",
      " - 10s - loss: 95.1618 - val_loss: 8.9627\n",
      "Epoch 457/500\n",
      " - 10s - loss: 95.1593 - val_loss: 8.9684\n",
      "Epoch 458/500\n",
      " - 10s - loss: 95.1573 - val_loss: 8.9638\n",
      "Epoch 459/500\n",
      " - 9s - loss: 95.1542 - val_loss: 8.9679\n",
      "Epoch 460/500\n",
      " - 8s - loss: 95.1529 - val_loss: 8.9645\n",
      "Epoch 461/500\n",
      " - 8s - loss: 95.1494 - val_loss: 8.9716\n",
      "Epoch 462/500\n",
      " - 8s - loss: 95.1483 - val_loss: 8.9710\n",
      "Epoch 463/500\n",
      " - 8s - loss: 95.1451 - val_loss: 8.9751\n",
      "Epoch 464/500\n",
      " - 8s - loss: 95.1426 - val_loss: 8.9725\n",
      "Epoch 465/500\n",
      " - 7s - loss: 95.1398 - val_loss: 8.9769\n",
      "Epoch 466/500\n",
      " - 7s - loss: 95.1375 - val_loss: 8.9713\n",
      "Epoch 467/500\n",
      " - 8s - loss: 95.1349 - val_loss: 8.9789\n",
      "Epoch 468/500\n",
      " - 8s - loss: 95.1333 - val_loss: 8.9760\n",
      "Epoch 469/500\n",
      " - 7s - loss: 95.1303 - val_loss: 8.9722\n",
      "Epoch 470/500\n",
      " - 8s - loss: 95.1272 - val_loss: 8.9768\n",
      "Epoch 471/500\n",
      " - 9s - loss: 95.1254 - val_loss: 8.9793\n",
      "Epoch 472/500\n",
      " - 8s - loss: 95.1379 - val_loss: 8.9781\n",
      "Epoch 473/500\n",
      " - 9s - loss: 100.8722 - val_loss: 74.7663\n",
      "Epoch 474/500\n",
      " - 9s - loss: 131.7982 - val_loss: 18.8855\n",
      "Epoch 475/500\n",
      " - 9s - loss: 100.0779 - val_loss: 9.2718\n",
      "Epoch 476/500\n",
      " - 9s - loss: 96.3214 - val_loss: 8.9850\n",
      "Epoch 477/500\n",
      " - 9s - loss: 96.1509 - val_loss: 8.9672\n",
      "Epoch 478/500\n",
      " - 9s - loss: 96.0941 - val_loss: 8.9689\n",
      "Epoch 479/500\n",
      " - 9s - loss: 96.0600 - val_loss: 8.9729\n",
      "Epoch 480/500\n",
      " - 9s - loss: 96.0367 - val_loss: 8.9744\n",
      "Epoch 481/500\n",
      " - 8s - loss: 96.0157 - val_loss: 8.9744\n",
      "Epoch 482/500\n",
      " - 9s - loss: 95.9945 - val_loss: 8.9749\n",
      "Epoch 483/500\n",
      " - 9s - loss: 95.9728 - val_loss: 8.9768\n",
      "Epoch 484/500\n",
      " - 9s - loss: 95.9494 - val_loss: 8.9652\n",
      "Epoch 485/500\n",
      " - 8s - loss: 95.9237 - val_loss: 8.9498\n",
      "Epoch 486/500\n",
      " - 8s - loss: 95.8963 - val_loss: 8.9576\n",
      "Epoch 487/500\n",
      " - 10s - loss: 95.8578 - val_loss: 8.9958\n",
      "Epoch 488/500\n",
      " - 12s - loss: 95.7784 - val_loss: 8.9902\n",
      "Epoch 489/500\n",
      " - 9s - loss: 95.7214 - val_loss: 8.9531\n",
      "Epoch 490/500\n",
      " - 9s - loss: 95.6792 - val_loss: 8.9261\n",
      "Epoch 491/500\n",
      " - 9s - loss: 95.6497 - val_loss: 8.9134\n",
      "Epoch 492/500\n",
      " - 8s - loss: 95.6260 - val_loss: 8.9141\n",
      "Epoch 493/500\n",
      " - 9s - loss: 95.6038 - val_loss: 8.9148\n",
      "Epoch 494/500\n",
      " - 8s - loss: 95.5885 - val_loss: 8.9113\n",
      "Epoch 495/500\n",
      " - 9s - loss: 95.5638 - val_loss: 8.9166\n",
      "Epoch 496/500\n",
      " - 9s - loss: 95.5472 - val_loss: 8.9167\n",
      "Epoch 497/500\n",
      " - 9s - loss: 95.5326 - val_loss: 8.9185\n",
      "Epoch 498/500\n",
      " - 7s - loss: 95.5175 - val_loss: 8.9218\n",
      "Epoch 499/500\n",
      " - 9s - loss: 95.5084 - val_loss: 8.9191\n",
      "Epoch 500/500\n",
      " - 8s - loss: 95.4924 - val_loss: 8.9247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f20782b6c50>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_split, y_split, epochs=500, validation_split=.2, shuffle=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(np.array([[dfaapl_clean.iloc[i:i+21].to_numpy()] for i in range(7)]).reshape(7,21,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.19 , -5.88 , -4.475,  1.54 , -1.8  , -1.92 ,  0.3  ])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.gradient(dfaapl_clean['AAPL close'])[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate prediction\n",
    "x_input = array([[70, 75], [80, 85], [90, 95]])\n",
    "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[70, 75],\n",
       "        [80, 85],\n",
       "        [90, 95]]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_arr = array([[70, 75], [80, 85], [90, 95]])\n",
    "test_arr_reshaped = x_input.reshape((1, 3, 2))\n",
    "test_arr_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.09145276,  0.08873188,  0.10333297, ..., -0.05087925,\n",
       "         -0.10069062,  0.1033785 ],\n",
       "        [-0.11160056, -0.06392359, -0.09621941, ..., -0.11491892,\n",
       "          0.00463326,  0.03331599],\n",
       "        [-0.06432565,  0.10922424,  0.02632555, ..., -0.06152768,\n",
       "         -0.03659218, -0.00981131],\n",
       "        ...,\n",
       "        [-0.08431162,  0.02128317,  0.06179332, ...,  0.04532259,\n",
       "         -0.10299102,  0.07582494],\n",
       "        [-0.0735914 , -0.11449953,  0.10863789, ...,  0.04448358,\n",
       "         -0.04677446, -0.10574875],\n",
       "        [-0.02573206, -0.06052234,  0.10493629, ..., -0.04483202,\n",
       "          0.00984795,  0.04517771]], dtype=float32),\n",
       " array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]], dtype=float32),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1., nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], dtype=float32),\n",
       " array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]], dtype=float32),\n",
       " array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]], dtype=float32),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1., nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        nan,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], dtype=float32),\n",
       " array([[nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan]], dtype=float32),\n",
       " array([nan, nan, nan, nan, nan, nan, nan], dtype=float32)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.775e+00, -8.450e-01, -7.950e-01, ..., -4.660e+00, -3.220e+00,\n",
       "         5.000e+00],\n",
       "       [-8.450e-01, -7.950e-01,  4.700e-01, ..., -3.220e+00,  5.000e+00,\n",
       "         5.080e+00],\n",
       "       [-7.950e-01,  4.700e-01, -4.660e+00, ...,  5.000e+00,  5.080e+00,\n",
       "         4.620e+00],\n",
       "       ...,\n",
       "       [-1.080e+00, -1.820e+00, -1.680e+00, ...,  3.500e-02, -7.000e-03,\n",
       "        -1.140e+00],\n",
       "       [-1.820e+00, -1.680e+00, -1.395e+00, ..., -7.000e-03, -1.140e+00,\n",
       "        -1.623e+00],\n",
       "       [-1.680e+00, -1.395e+00,  3.500e-02, ..., -1.140e+00, -1.623e+00,\n",
       "        -1.080e+00]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-15., -14., -12., -10.,  -8.,  -6.,  -4.,  -2.,   0.,   2.,   4.,\n",
       "         6.,   8.,  10.,  12.,  14.,  15.])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_arr = np.array([64,49,36,25,16,9,4,1,0,1,4,9,16,25,36,49,64])\n",
    "np.gradient(test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0567525 , 0.03734604, 0.03070688, 0.03070688, 0.03070688,\n",
       "       0.03070688, 0.030707  ], dtype=float32)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALLOW_THREADS',\n",
       " 'AxisError',\n",
       " 'BUFSIZE',\n",
       " 'CLIP',\n",
       " 'ComplexWarning',\n",
       " 'DataSource',\n",
       " 'ERR_CALL',\n",
       " 'ERR_DEFAULT',\n",
       " 'ERR_IGNORE',\n",
       " 'ERR_LOG',\n",
       " 'ERR_PRINT',\n",
       " 'ERR_RAISE',\n",
       " 'ERR_WARN',\n",
       " 'FLOATING_POINT_SUPPORT',\n",
       " 'FPE_DIVIDEBYZERO',\n",
       " 'FPE_INVALID',\n",
       " 'FPE_OVERFLOW',\n",
       " 'FPE_UNDERFLOW',\n",
       " 'False_',\n",
       " 'Inf',\n",
       " 'Infinity',\n",
       " 'LowLevelCallable',\n",
       " 'MAXDIMS',\n",
       " 'MAY_SHARE_BOUNDS',\n",
       " 'MAY_SHARE_EXACT',\n",
       " 'MachAr',\n",
       " 'ModuleDeprecationWarning',\n",
       " 'NAN',\n",
       " 'NINF',\n",
       " 'NZERO',\n",
       " 'NaN',\n",
       " 'PINF',\n",
       " 'PZERO',\n",
       " 'PackageLoader',\n",
       " 'RAISE',\n",
       " 'RankWarning',\n",
       " 'SHIFT_DIVIDEBYZERO',\n",
       " 'SHIFT_INVALID',\n",
       " 'SHIFT_OVERFLOW',\n",
       " 'SHIFT_UNDERFLOW',\n",
       " 'ScalarType',\n",
       " 'TooHardError',\n",
       " 'True_',\n",
       " 'UFUNC_BUFSIZE_DEFAULT',\n",
       " 'UFUNC_PYVALS_NAME',\n",
       " 'VisibleDeprecationWarning',\n",
       " 'WRAP',\n",
       " '__SCIPY_SETUP__',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__config__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__numpy_version__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_distributor_init',\n",
       " '_lib',\n",
       " 'absolute',\n",
       " 'absolute_import',\n",
       " 'add',\n",
       " 'add_docstring',\n",
       " 'add_newdoc',\n",
       " 'add_newdoc_ufunc',\n",
       " 'add_newdocs',\n",
       " 'alen',\n",
       " 'all',\n",
       " 'allclose',\n",
       " 'alltrue',\n",
       " 'amax',\n",
       " 'amin',\n",
       " 'angle',\n",
       " 'any',\n",
       " 'append',\n",
       " 'apply_along_axis',\n",
       " 'apply_over_axes',\n",
       " 'arange',\n",
       " 'arccos',\n",
       " 'arccosh',\n",
       " 'arcsin',\n",
       " 'arcsinh',\n",
       " 'arctan',\n",
       " 'arctan2',\n",
       " 'arctanh',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'argpartition',\n",
       " 'argsort',\n",
       " 'argwhere',\n",
       " 'around',\n",
       " 'array',\n",
       " 'array2string',\n",
       " 'array_equal',\n",
       " 'array_equiv',\n",
       " 'array_repr',\n",
       " 'array_split',\n",
       " 'array_str',\n",
       " 'asanyarray',\n",
       " 'asarray',\n",
       " 'asarray_chkfinite',\n",
       " 'ascontiguousarray',\n",
       " 'asfarray',\n",
       " 'asfortranarray',\n",
       " 'asmatrix',\n",
       " 'asscalar',\n",
       " 'atleast_1d',\n",
       " 'atleast_2d',\n",
       " 'atleast_3d',\n",
       " 'average',\n",
       " 'bartlett',\n",
       " 'base_repr',\n",
       " 'binary_repr',\n",
       " 'bincount',\n",
       " 'bitwise_and',\n",
       " 'bitwise_not',\n",
       " 'bitwise_or',\n",
       " 'bitwise_xor',\n",
       " 'blackman',\n",
       " 'block',\n",
       " 'bmat',\n",
       " 'bool8',\n",
       " 'bool_',\n",
       " 'broadcast',\n",
       " 'broadcast_arrays',\n",
       " 'broadcast_to',\n",
       " 'busday_count',\n",
       " 'busday_offset',\n",
       " 'busdaycalendar',\n",
       " 'byte',\n",
       " 'byte_bounds',\n",
       " 'bytes0',\n",
       " 'bytes_',\n",
       " 'c_',\n",
       " 'can_cast',\n",
       " 'cast',\n",
       " 'cbrt',\n",
       " 'cdouble',\n",
       " 'ceil',\n",
       " 'cfloat',\n",
       " 'char',\n",
       " 'character',\n",
       " 'chararray',\n",
       " 'choose',\n",
       " 'clip',\n",
       " 'clongdouble',\n",
       " 'clongfloat',\n",
       " 'cluster',\n",
       " 'column_stack',\n",
       " 'common_type',\n",
       " 'compare_chararrays',\n",
       " 'complex128',\n",
       " 'complex256',\n",
       " 'complex64',\n",
       " 'complex_',\n",
       " 'complexfloating',\n",
       " 'compress',\n",
       " 'concatenate',\n",
       " 'conj',\n",
       " 'conjugate',\n",
       " 'convolve',\n",
       " 'copy',\n",
       " 'copysign',\n",
       " 'copyto',\n",
       " 'corrcoef',\n",
       " 'correlate',\n",
       " 'cos',\n",
       " 'cosh',\n",
       " 'count_nonzero',\n",
       " 'cov',\n",
       " 'cross',\n",
       " 'csingle',\n",
       " 'ctypeslib',\n",
       " 'cumprod',\n",
       " 'cumproduct',\n",
       " 'cumsum',\n",
       " 'datetime64',\n",
       " 'datetime_as_string',\n",
       " 'datetime_data',\n",
       " 'deg2rad',\n",
       " 'degrees',\n",
       " 'delete',\n",
       " 'deprecate',\n",
       " 'deprecate_with_doc',\n",
       " 'diag',\n",
       " 'diag_indices',\n",
       " 'diag_indices_from',\n",
       " 'diagflat',\n",
       " 'diagonal',\n",
       " 'diff',\n",
       " 'digitize',\n",
       " 'disp',\n",
       " 'divide',\n",
       " 'division',\n",
       " 'divmod',\n",
       " 'dot',\n",
       " 'double',\n",
       " 'dsplit',\n",
       " 'dstack',\n",
       " 'dtype',\n",
       " 'e',\n",
       " 'ediff1d',\n",
       " 'einsum',\n",
       " 'einsum_path',\n",
       " 'emath',\n",
       " 'empty',\n",
       " 'empty_like',\n",
       " 'equal',\n",
       " 'errstate',\n",
       " 'euler_gamma',\n",
       " 'exp',\n",
       " 'exp2',\n",
       " 'expand_dims',\n",
       " 'expm1',\n",
       " 'extract',\n",
       " 'eye',\n",
       " 'fabs',\n",
       " 'fastCopyAndTranspose',\n",
       " 'fft',\n",
       " 'fill_diagonal',\n",
       " 'find_common_type',\n",
       " 'finfo',\n",
       " 'fix',\n",
       " 'flatiter',\n",
       " 'flatnonzero',\n",
       " 'flexible',\n",
       " 'flip',\n",
       " 'fliplr',\n",
       " 'flipud',\n",
       " 'float128',\n",
       " 'float16',\n",
       " 'float32',\n",
       " 'float64',\n",
       " 'float_',\n",
       " 'float_power',\n",
       " 'floating',\n",
       " 'floor',\n",
       " 'floor_divide',\n",
       " 'fmax',\n",
       " 'fmin',\n",
       " 'fmod',\n",
       " 'format_float_positional',\n",
       " 'format_float_scientific',\n",
       " 'format_parser',\n",
       " 'frexp',\n",
       " 'frombuffer',\n",
       " 'fromfile',\n",
       " 'fromfunction',\n",
       " 'fromiter',\n",
       " 'frompyfunc',\n",
       " 'fromregex',\n",
       " 'fromstring',\n",
       " 'full',\n",
       " 'full_like',\n",
       " 'fv',\n",
       " 'generic',\n",
       " 'genfromtxt',\n",
       " 'geomspace',\n",
       " 'get_array_wrap',\n",
       " 'get_include',\n",
       " 'get_printoptions',\n",
       " 'getbufsize',\n",
       " 'geterr',\n",
       " 'geterrcall',\n",
       " 'geterrobj',\n",
       " 'gradient',\n",
       " 'greater',\n",
       " 'greater_equal',\n",
       " 'half',\n",
       " 'hamming',\n",
       " 'hanning',\n",
       " 'heaviside',\n",
       " 'histogram',\n",
       " 'histogram2d',\n",
       " 'histogramdd',\n",
       " 'hsplit',\n",
       " 'hstack',\n",
       " 'hypot',\n",
       " 'i0',\n",
       " 'identity',\n",
       " 'ifft',\n",
       " 'iinfo',\n",
       " 'imag',\n",
       " 'in1d',\n",
       " 'index_exp',\n",
       " 'indices',\n",
       " 'inexact',\n",
       " 'inf',\n",
       " 'info',\n",
       " 'infty',\n",
       " 'inner',\n",
       " 'insert',\n",
       " 'int0',\n",
       " 'int16',\n",
       " 'int32',\n",
       " 'int64',\n",
       " 'int8',\n",
       " 'int_',\n",
       " 'int_asbuffer',\n",
       " 'intc',\n",
       " 'integer',\n",
       " 'integrate',\n",
       " 'interp',\n",
       " 'interpolate',\n",
       " 'intersect1d',\n",
       " 'intp',\n",
       " 'invert',\n",
       " 'io',\n",
       " 'ipmt',\n",
       " 'irr',\n",
       " 'is_busday',\n",
       " 'isclose',\n",
       " 'iscomplex',\n",
       " 'iscomplexobj',\n",
       " 'isfinite',\n",
       " 'isfortran',\n",
       " 'isin',\n",
       " 'isinf',\n",
       " 'isnan',\n",
       " 'isnat',\n",
       " 'isneginf',\n",
       " 'isposinf',\n",
       " 'isreal',\n",
       " 'isrealobj',\n",
       " 'isscalar',\n",
       " 'issctype',\n",
       " 'issubclass_',\n",
       " 'issubdtype',\n",
       " 'issubsctype',\n",
       " 'iterable',\n",
       " 'ix_',\n",
       " 'kaiser',\n",
       " 'kron',\n",
       " 'ldexp',\n",
       " 'left_shift',\n",
       " 'less',\n",
       " 'less_equal',\n",
       " 'lexsort',\n",
       " 'linalg',\n",
       " 'linspace',\n",
       " 'little_endian',\n",
       " 'load',\n",
       " 'loads',\n",
       " 'loadtxt',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log1p',\n",
       " 'log2',\n",
       " 'logaddexp',\n",
       " 'logaddexp2',\n",
       " 'logical_and',\n",
       " 'logical_not',\n",
       " 'logical_or',\n",
       " 'logical_xor',\n",
       " 'logn',\n",
       " 'logspace',\n",
       " 'long',\n",
       " 'longcomplex',\n",
       " 'longdouble',\n",
       " 'longfloat',\n",
       " 'longlong',\n",
       " 'lookfor',\n",
       " 'ma',\n",
       " 'mafromtxt',\n",
       " 'mask_indices',\n",
       " 'mat',\n",
       " 'math',\n",
       " 'matmul',\n",
       " 'matrix',\n",
       " 'maximum',\n",
       " 'maximum_sctype',\n",
       " 'may_share_memory',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'memmap',\n",
       " 'meshgrid',\n",
       " 'mgrid',\n",
       " 'min_scalar_type',\n",
       " 'minimum',\n",
       " 'mintypecode',\n",
       " 'mirr',\n",
       " 'misc',\n",
       " 'mod',\n",
       " 'modf',\n",
       " 'moveaxis',\n",
       " 'msort',\n",
       " 'multiply',\n",
       " 'nan',\n",
       " 'nan_to_num',\n",
       " 'nanargmax',\n",
       " 'nanargmin',\n",
       " 'nancumprod',\n",
       " 'nancumsum',\n",
       " 'nanmax',\n",
       " 'nanmean',\n",
       " 'nanmedian',\n",
       " 'nanmin',\n",
       " 'nanpercentile',\n",
       " 'nanprod',\n",
       " 'nanstd',\n",
       " 'nansum',\n",
       " 'nanvar',\n",
       " 'nbytes',\n",
       " 'ndarray',\n",
       " 'ndenumerate',\n",
       " 'ndfromtxt',\n",
       " 'ndim',\n",
       " 'ndimage',\n",
       " 'ndindex',\n",
       " 'nditer',\n",
       " 'negative',\n",
       " 'nested_iters',\n",
       " 'newaxis',\n",
       " 'nextafter',\n",
       " 'nonzero',\n",
       " 'not_equal',\n",
       " 'nper',\n",
       " 'npv',\n",
       " 'number',\n",
       " 'obj2sctype',\n",
       " 'object0',\n",
       " 'object_',\n",
       " 'ogrid',\n",
       " 'ones',\n",
       " 'ones_like',\n",
       " 'optimize',\n",
       " 'outer',\n",
       " 'packbits',\n",
       " 'pad',\n",
       " 'partition',\n",
       " 'percentile',\n",
       " 'pi',\n",
       " 'piecewise',\n",
       " 'pkgload',\n",
       " 'place',\n",
       " 'pmt',\n",
       " 'poly',\n",
       " 'poly1d',\n",
       " 'polyadd',\n",
       " 'polyder',\n",
       " 'polydiv',\n",
       " 'polyfit',\n",
       " 'polyint',\n",
       " 'polymul',\n",
       " 'polysub',\n",
       " 'polyval',\n",
       " 'positive',\n",
       " 'power',\n",
       " 'ppmt',\n",
       " 'print_function',\n",
       " 'prod',\n",
       " 'product',\n",
       " 'promote_types',\n",
       " 'ptp',\n",
       " 'put',\n",
       " 'putmask',\n",
       " 'pv',\n",
       " 'r_',\n",
       " 'rad2deg',\n",
       " 'radians',\n",
       " 'rand',\n",
       " 'randn',\n",
       " 'random',\n",
       " 'rank',\n",
       " 'rate',\n",
       " 'ravel',\n",
       " 'ravel_multi_index',\n",
       " 'real',\n",
       " 'real_if_close',\n",
       " 'rec',\n",
       " 'recarray',\n",
       " 'recfromcsv',\n",
       " 'recfromtxt',\n",
       " 'reciprocal',\n",
       " 'record',\n",
       " 'remainder',\n",
       " 'repeat',\n",
       " 'require',\n",
       " 'reshape',\n",
       " 'resize',\n",
       " 'result_type',\n",
       " 'right_shift',\n",
       " 'rint',\n",
       " 'roll',\n",
       " 'rollaxis',\n",
       " 'roots',\n",
       " 'rot90',\n",
       " 'round_',\n",
       " 'row_stack',\n",
       " 's_',\n",
       " 'safe_eval',\n",
       " 'save',\n",
       " 'savetxt',\n",
       " 'savez',\n",
       " 'savez_compressed',\n",
       " 'sctype2char',\n",
       " 'sctypeDict',\n",
       " 'sctypeNA',\n",
       " 'sctypes',\n",
       " 'searchsorted',\n",
       " 'select',\n",
       " 'set_numeric_ops',\n",
       " 'set_printoptions',\n",
       " 'set_string_function',\n",
       " 'setbufsize',\n",
       " 'setdiff1d',\n",
       " 'seterr',\n",
       " 'seterrcall',\n",
       " 'seterrobj',\n",
       " 'setxor1d',\n",
       " 'shape',\n",
       " 'shares_memory',\n",
       " 'short',\n",
       " 'show_config',\n",
       " 'show_numpy_config',\n",
       " 'sign',\n",
       " 'signbit',\n",
       " 'signedinteger',\n",
       " 'sin',\n",
       " 'sinc',\n",
       " 'single',\n",
       " 'singlecomplex',\n",
       " 'sinh',\n",
       " 'size',\n",
       " 'sometrue',\n",
       " 'sort',\n",
       " 'sort_complex',\n",
       " 'source',\n",
       " 'spacing',\n",
       " 'sparse',\n",
       " 'spatial',\n",
       " 'special',\n",
       " 'split',\n",
       " 'sqrt',\n",
       " 'square',\n",
       " 'squeeze',\n",
       " 'stack',\n",
       " 'stats',\n",
       " 'std',\n",
       " 'str0',\n",
       " 'str_',\n",
       " 'string_',\n",
       " 'subtract',\n",
       " 'sum',\n",
       " 'swapaxes',\n",
       " 'take',\n",
       " 'tan',\n",
       " 'tanh',\n",
       " 'tensordot',\n",
       " 'test',\n",
       " 'tile',\n",
       " 'timedelta64',\n",
       " 'trace',\n",
       " 'tracemalloc_domain',\n",
       " 'transpose',\n",
       " 'trapz',\n",
       " 'tri',\n",
       " 'tril',\n",
       " 'tril_indices',\n",
       " 'tril_indices_from',\n",
       " 'trim_zeros',\n",
       " 'triu',\n",
       " 'triu_indices',\n",
       " 'triu_indices_from',\n",
       " 'true_divide',\n",
       " 'trunc',\n",
       " 'typeDict',\n",
       " 'typeNA',\n",
       " 'typecodes',\n",
       " 'typename',\n",
       " 'ubyte',\n",
       " 'ufunc',\n",
       " 'uint',\n",
       " 'uint0',\n",
       " 'uint16',\n",
       " 'uint32',\n",
       " 'uint64',\n",
       " 'uint8',\n",
       " 'uintc',\n",
       " 'uintp',\n",
       " 'ulonglong',\n",
       " 'unicode',\n",
       " 'unicode_',\n",
       " 'union1d',\n",
       " 'unique',\n",
       " 'unpackbits',\n",
       " 'unravel_index',\n",
       " 'unsignedinteger',\n",
       " 'unwrap',\n",
       " 'ushort',\n",
       " 'vander',\n",
       " 'var',\n",
       " 'vdot',\n",
       " 'vectorize',\n",
       " 'version',\n",
       " 'void',\n",
       " 'void0',\n",
       " 'vsplit',\n",
       " 'vstack',\n",
       " 'where',\n",
       " 'who',\n",
       " 'zeros',\n",
       " 'zeros_like']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fracdiff import FractionalDifferentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
